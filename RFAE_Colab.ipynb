{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cc6645",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸžï¸ Random-Forest Autoencoder (RFAE) â€” Colab Notebook\n",
    "\n",
    "This notebook demonstrates training, encoding, decoding, and evaluation for **Autoencoding Random Forests (RFAE)** in Python using **PyTorch + scikit-learn**.\n",
    "\n",
    "**References**\n",
    "- *Autoencoding Random Forests* â€” Binh Duc Vu, Jan Kapar, Marvin N. Wright, David S. Watson (arXiv:2505.21441, 2025).\n",
    "- Official research repo (R): https://github.com/bips-hb/RFAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ðŸ”§ Setup: install dependencies\n",
    "!pip -q install numpy pandas scikit-learn torch scipy matplotlib\n",
    "print(\"âœ… Dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cbb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ðŸ“¦ Create the local module `rfae.py` (auto-generated)\n",
    "%%writefile rfae.py\n",
    "# rfae.py\n",
    "# MIT License (c) 2025\n",
    "\"\"\"\n",
    "Random-Forest Autoencoder (RFAE) â€” PyTorch + scikit-learn implementation\n",
    "\n",
    "Encoder: Diffusion-map embedding on a Random-Forest (RF) kernel with NystrÃ¶m out-of-sample.\n",
    "Decoder: k-NN in latent space (weighted mean for numeric; weighted vote for categoricals).\n",
    "Optional: exact-but-slower synthetic representatives via leaf-box intersections.\n",
    "Optional: tiny neural encoder (Torch MLP) to approximate NystrÃ¶m for high-throughput inference.\n",
    "\n",
    "References\n",
    "---------\n",
    "- Binh Duc Vu, Jan Kapar, Marvin N. Wright, David S. Watson. \"Autoencoding Random Forests.\"\n",
    "  arXiv:2505.21441 (2025).\n",
    "- Official R/Python reference repo: https://github.com/bips-hb/RFAE\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import dataclasses\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    RandomTreesEmbedding,\n",
    ")\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from scipy.sparse.linalg import eigsh  # type: ignore\n",
    "    SCIPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RFAEConfig:\n",
    "    # Forest\n",
    "    n_estimators: int = 500\n",
    "    max_depth: Optional[int] = None\n",
    "    min_samples_leaf: int = 1\n",
    "    random_state: Optional[int] = 42\n",
    "    n_jobs: Optional[int] = None\n",
    "\n",
    "    # Mode: \"supervised-classification\", \"supervised-regression\", \"unsupervised\"\n",
    "    mode: str = \"supervised-classification\"\n",
    "\n",
    "    # Diffusion map\n",
    "    latent_dim: int = 16\n",
    "    diffusion_time: float = 1.0\n",
    "    use_torch: bool = True\n",
    "    device: Optional[str] = None\n",
    "\n",
    "    # Decoder (k-NN)\n",
    "    k_neighbors: int = 50\n",
    "    distance_eps: float = 1e-8\n",
    "    weight_power: float = 2.0\n",
    "    exact_decoder: bool = False\n",
    "    max_trees_for_intersection: Optional[int] = 256\n",
    "\n",
    "    # Preprocessing\n",
    "    standardize_numeric: bool = True\n",
    "    categorical_strategy: str = \"ordinal\"\n",
    "    handle_unknown_cats: str = \"use_encoded_value\"\n",
    "\n",
    "    # Numerics\n",
    "    eigen_solver: str = \"auto\"  # \"auto\" | \"numpy\" | \"scipy\" | \"torch\"\n",
    "    float_dtype: str = \"float64\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "\n",
    "def _infer_columns(X: Union[pd.DataFrame, np.ndarray]) -> Tuple[List[str], List[str]]:\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        cat_cols = [c for c in X.columns\n",
    "                    if pd.api.types.is_object_dtype(X[c]) or pd.api.types.is_categorical_dtype(X[c])]\n",
    "        num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "        return num_cols, cat_cols\n",
    "    else:\n",
    "        n = X.shape[1]\n",
    "        return [f\"x{i}\" for i in range(n)], []\n",
    "\n",
    "\n",
    "def _make_preprocessor(X: Union[pd.DataFrame, np.ndarray], cfg: RFAEConfig) -> Tuple[Pipeline, List[str], List[str]]:\n",
    "    num_cols, cat_cols = _infer_columns(X)\n",
    "    transformers = []\n",
    "    if len(num_cols) > 0:\n",
    "        transformers.append((\"num\", StandardScaler() if cfg.standardize_numeric else \"passthrough\", num_cols))\n",
    "    if len(cat_cols) > 0:\n",
    "        if cfg.categorical_strategy == \"ordinal\":\n",
    "            oe = OrdinalEncoder(handle_unknown=cfg.handle_unknown_cats, unknown_value=-1)\n",
    "            transformers.append((\"cat\", oe, cat_cols))\n",
    "        else:\n",
    "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "            transformers.append((\"cat\", ohe, cat_cols))\n",
    "    pre = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "    return Pipeline([(\"pre\", pre)]), num_cols, cat_cols\n",
    "\n",
    "\n",
    "class RFAE(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, config: Optional[RFAEConfig] = None):\n",
    "        self.cfg = config or RFAEConfig()\n",
    "        self._rng = check_random_state(self.cfg.random_state)\n",
    "\n",
    "        self.preprocessor_: Optional[Pipeline] = None\n",
    "        self.num_cols_: List[str] = []\n",
    "        self.cat_cols_: List[str] = []\n",
    "        self.is_supervised_: bool = False\n",
    "        self.task_: str = \"classification\"\n",
    "\n",
    "        self.forest_: Any = None\n",
    "        self.estimators_: List[Any] = []\n",
    "\n",
    "        self.K_: Optional[np.ndarray] = None\n",
    "        self.evals_: Optional[np.ndarray] = None\n",
    "        self.evecs_: Optional[np.ndarray] = None\n",
    "        self.Z_train_: Optional[np.ndarray] = None\n",
    "        self.Z_scaler_: float = 1.0\n",
    "\n",
    "        self.X_train_pre_: Optional[np.ndarray] = None\n",
    "        self.X_train_df_: Optional[pd.DataFrame] = None\n",
    "        self.n_samples_: int = 0\n",
    "        self.n_features_: int = 0\n",
    "\n",
    "        self._leaf_index_per_tree_: List[Dict[int, np.ndarray]] = []\n",
    "        self._leaf_size_per_tree_: List[Dict[int, int]] = []\n",
    "\n",
    "        self._synthetic_train_X_: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Optional[Union[np.ndarray, Iterable]] = None) -> \"RFAE\":\n",
    "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=[f\"x{i}\" for i in range(np.asarray(X).shape[1])])\n",
    "        self.X_train_df_ = X_df.copy()\n",
    "\n",
    "        self.preprocessor_, self.num_cols_, self.cat_cols_ = _make_preprocessor(X_df, self.cfg)\n",
    "        Xp = np.asarray(self.preprocessor_.fit_transform(X_df), dtype=self.cfg.float_dtype)\n",
    "        self.X_train_pre_ = Xp\n",
    "        self.n_samples_, self.n_features_ = Xp.shape\n",
    "\n",
    "        self.is_supervised_ = self.cfg.mode.startswith(\"supervised\")\n",
    "        if self.is_supervised_:\n",
    "            if y is None:\n",
    "                raise ValueError(\"Supervised mode selected but y is None.\")\n",
    "            y = np.asarray(y)\n",
    "            if self.cfg.mode == \"supervised-classification\":\n",
    "                self.task_ = \"classification\"\n",
    "                self.forest_ = RandomForestClassifier(\n",
    "                    n_estimators=self.cfg.n_estimators,\n",
    "                    max_depth=self.cfg.max_depth,\n",
    "                    min_samples_leaf=self.cfg.min_samples_leaf,\n",
    "                    n_jobs=self.cfg.n_jobs,\n",
    "                    random_state=self.cfg.random_state,\n",
    "                )\n",
    "            elif self.cfg.mode == \"supervised-regression\":\n",
    "                self.task_ = \"regression\"\n",
    "                self.forest_ = RandomForestRegressor(\n",
    "                    n_estimators=self.cfg.n_estimators,\n",
    "                    max_depth=self.cfg.max_depth,\n",
    "                    min_samples_leaf=self.cfg.min_samples_leaf,\n",
    "                    n_jobs=self.cfg.n_jobs,\n",
    "                    random_state=self.cfg.random_state,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown supervised mode: {self.cfg.mode}\")\n",
    "            self.forest_.fit(Xp, y)\n",
    "            self.estimators_ = list(self.forest_.estimators_)\n",
    "        else:\n",
    "            self.task_ = \"unsupervised\"\n",
    "            rte = RandomTreesEmbedding(\n",
    "                n_estimators=self.cfg.n_estimators,\n",
    "                max_depth=self.cfg.max_depth,\n",
    "                n_jobs=self.cfg.n_jobs,\n",
    "                random_state=self.cfg.random_state,\n",
    "            )\n",
    "            rte.fit(Xp)\n",
    "            self.forest_ = rte\n",
    "            self.estimators_ = list(rte.estimators_)\n",
    "\n",
    "        if self.cfg.verbose:\n",
    "            print(f\"[RFAE] Fitted forest with {len(self.estimators_)} trees; building RF kernel...\")\n",
    "\n",
    "        self._leaf_index_per_tree_.clear()\n",
    "        self._leaf_size_per_tree_.clear()\n",
    "        for est in self.estimators_:\n",
    "            leaves = est.apply(self.X_train_pre_)\n",
    "            leaf_to_idx = {int(lid): np.where(leaves == lid)[0] for lid in np.unique(leaves)}\n",
    "            self._leaf_index_per_tree_.append(leaf_to_idx)\n",
    "            self._leaf_size_per_tree_.append({lid: len(idx) for lid, idx in leaf_to_idx.items()})\n",
    "\n",
    "        self.K_ = self._build_rf_kernel_train()\n",
    "        if self.cfg.verbose:\n",
    "            print(\"[RFAE] RF kernel built. Row sum (first 5):\", np.round(self.K_.sum(1)[:5], 3))\n",
    "\n",
    "        self.evals_, self.evecs_ = self._eigendecompose(self.K_, top_k=self.cfg.latent_dim + 1)\n",
    "        evals_nt = self.evals_[1: self.cfg.latent_dim + 1]\n",
    "        evecs_nt = self.evecs_[:, 1: self.cfg.latent_dim + 1]\n",
    "        self.Z_scaler_ = float(np.sqrt(self.n_samples_))\n",
    "        self.Z_train_ = (self.Z_scaler_ * evecs_nt) * (evals_nt ** self.cfg.diffusion_time)\n",
    "\n",
    "        if self.cfg.exact_decoder:\n",
    "            if self.cfg.verbose:\n",
    "                print(\"[RFAE] Precomputing synthetic representatives (exact decoder)...\")\n",
    "            self._synthetic_train_X_ = self._build_synthetic_training_points()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def encode(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        check_is_fitted(self, [\"Z_train_\", \"evals_\", \"evecs_\", \"K_\"])\n",
    "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.X_train_df_.columns)\n",
    "        Xp = np.asarray(self.preprocessor_.transform(X_df), dtype=self.cfg.float_dtype)\n",
    "\n",
    "        K0 = self._build_rf_kernel_test(Xp)\n",
    "        evals_nt = self.evals_[1: self.cfg.latent_dim + 1]\n",
    "        Z0 = K0 @ (self.Z_train_ * (evals_nt ** -1.0))\n",
    "        return np.asarray(Z0, dtype=self.cfg.float_dtype)\n",
    "\n",
    "    def decode(self, Z: np.ndarray, k: Optional[int] = None) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        check_is_fitted(self, [\"Z_train_\", \"X_train_pre_\"])\n",
    "        k = k or self.cfg.k_neighbors\n",
    "        Z = np.asarray(Z, dtype=self.cfg.float_dtype)\n",
    "\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(k, len(self.Z_train_))).fit(self.Z_train_)\n",
    "        dists, inds = nbrs.kneighbors(Z, return_distance=True)\n",
    "\n",
    "        w = 1.0 / np.maximum(dists, self.cfg.distance_eps) ** self.cfg.weight_power\n",
    "        w = (w / w.sum(axis=1, keepdims=True)).astype(self.cfg.float_dtype)\n",
    "\n",
    "        Xrep = self._synthetic_train_X_ if (self.cfg.exact_decoder and self._synthetic_train_X_ is not None) else self.X_train_pre_\n",
    "        X_rep_neighbors = Xrep[inds]\n",
    "        X_cont_recon = np.einsum(\"mk,mkd->md\", w, X_rep_neighbors)\n",
    "\n",
    "        out_df = pd.DataFrame(index=range(Z.shape[0]))\n",
    "        if len(self.num_cols_) > 0:\n",
    "            n_num = len(self.num_cols_)\n",
    "            out_df[self.num_cols_] = X_cont_recon[:, :n_num]\n",
    "\n",
    "        if len(self.cat_cols_) > 0:\n",
    "            X_train_raw = self.X_train_df_.reset_index(drop=True)\n",
    "            cat_vals_neighbors = X_train_raw.loc[inds.flatten(), self.cat_cols_].to_numpy().reshape(Z.shape[0], -1, len(self.cat_cols_))\n",
    "            for j, col in enumerate(self.cat_cols_):\n",
    "                winners = []\n",
    "                for i in range(Z.shape[0]):\n",
    "                    label_to_w = {}\n",
    "                    for lbl, ww in zip(cat_vals_neighbors[i, :, j], w[i]):\n",
    "                        label_to_w[lbl] = label_to_w.get(lbl, 0.0) + float(ww)\n",
    "                    top = max(label_to_w.values())\n",
    "                    cands = [lbl for lbl, ww in label_to_w.items() if abs(ww - top) < 1e-12]\n",
    "                    winners.append(self._rng.choice(cands))\n",
    "                out_df[col] = np.array(winners, dtype=object)\n",
    "        return out_df\n",
    "\n",
    "    def _build_rf_kernel_train(self) -> np.ndarray:\n",
    "        n = self.n_samples_\n",
    "        K = np.zeros((n, n), dtype=self.cfg.float_dtype)\n",
    "        B = float(len(self.estimators_))\n",
    "        for leaf_to_idx in self._leaf_index_per_tree_:\n",
    "            for lid, idx in leaf_to_idx.items():\n",
    "                sz = len(idx)\n",
    "                if sz > 0:\n",
    "                    K[np.ix_(idx, idx)] += (1.0 / sz)\n",
    "        K /= B\n",
    "        K = 0.5 * (K + K.T)\n",
    "        K[K < 0] = 0.0\n",
    "        return K\n",
    "\n",
    "    def _build_rf_kernel_test(self, X_test_pre: np.ndarray) -> np.ndarray:\n",
    "        m, n = X_test_pre.shape[0], self.n_samples_\n",
    "        K0 = np.zeros((m, n), dtype=self.cfg.float_dtype)\n",
    "        B = float(len(self.estimators_))\n",
    "        for est, leaf_to_idx, leaf_sizes in zip(self.estimators_, self._leaf_index_per_tree_, self._leaf_size_per_tree_):\n",
    "            test_leaves = est.apply(X_test_pre)\n",
    "            for i, lid in enumerate(test_leaves):\n",
    "                idx = leaf_to_idx.get(int(lid))\n",
    "                if idx is None:\n",
    "                    continue\n",
    "                sz = leaf_sizes[int(lid)]\n",
    "                if sz > 0:\n",
    "                    K0[i, idx] += (1.0 / sz)\n",
    "        K0 /= B\n",
    "        row_sums = K0.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        K0 /= row_sums\n",
    "        return K0\n",
    "\n",
    "    def _eigendecompose(self, K: np.ndarray, top_k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        solver = self.cfg.eigen_solver\n",
    "        if solver == \"auto\":\n",
    "            if TORCH_AVAILABLE and self.cfg.use_torch:\n",
    "                solver = \"torch\"\n",
    "            elif SCIPY_AVAILABLE and K.shape[0] >= 512:\n",
    "                solver = \"scipy\"\n",
    "            else:\n",
    "                solver = \"numpy\"\n",
    "\n",
    "        if solver == \"torch\" and TORCH_AVAILABLE:\n",
    "            device = self.cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            tK = torch.tensor(K, dtype=torch.float64 if self.cfg.float_dtype == \"float64\" else torch.float32, device=device)\n",
    "            evals, evecs = torch.linalg.eigh(tK)\n",
    "            evals = evals.detach().cpu().numpy()\n",
    "            evecs = evecs.detach().cpu().numpy()\n",
    "        elif solver == \"scipy\" and SCIPY_AVAILABLE:\n",
    "            k = min(top_k, K.shape[0] - 1)\n",
    "            evals, evecs = eigsh(K, k=k, which=\"LA\")\n",
    "        else:\n",
    "            evals, evecs = np.linalg.eigh(K)\n",
    "\n",
    "        idx = np.argsort(evals)[::-1]\n",
    "        return evals[idx], evecs[:, idx]\n",
    "\n",
    "    def _build_synthetic_training_points(self) -> np.ndarray:\n",
    "        Xp = self.X_train_pre_\n",
    "        n, d = Xp.shape\n",
    "        global_min = np.min(Xp, axis=0)\n",
    "        global_max = np.max(Xp, axis=0)\n",
    "        trees = self.estimators_[: self.cfg.max_trees_for_intersection] if self.cfg.max_trees_for_intersection else self.estimators_\n",
    "        lows = np.tile(global_min, (n, 1)).astype(self.cfg.float_dtype)\n",
    "        highs = np.tile(global_max, (n, 1)).astype(self.cfg.float_dtype)\n",
    "\n",
    "        for est in trees:\n",
    "            tree = est.tree_\n",
    "            path = est.decision_path(Xp)\n",
    "            features = tree.feature\n",
    "            thresholds = tree.threshold\n",
    "            for i in range(n):\n",
    "                node_indices = path.indices[path.indptr[i]: path.indptr[i+1]]\n",
    "                for node in node_indices:\n",
    "                    feat = features[node]\n",
    "                    if feat < 0:\n",
    "                        continue\n",
    "                    thr = thresholds[node]\n",
    "                    if Xp[i, feat] <= thr:\n",
    "                        highs[i, feat] = min(highs[i, feat], thr)\n",
    "                    else:\n",
    "                        import numpy as _np\n",
    "                        lows[i, feat] = max(lows[i, feat], _np.nextafter(thr, _np.inf))\n",
    "\n",
    "        lows = np.minimum(lows, highs)\n",
    "        widths = np.maximum(highs - lows, 0.0)\n",
    "        U = self._rng.uniform(size=(n, d)).astype(self.cfg.float_dtype)\n",
    "        return lows + U * widths\n",
    "\n",
    "    def save(self, path: Union[str, 'Path']) -> None:\n",
    "        from pathlib import Path as _Path\n",
    "        check_is_fitted(self, [\"Z_train_\", \"evecs_\", \"evals_\"])\n",
    "        path = _Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        meta = {\n",
    "            \"cfg\": dataclasses.asdict(self.cfg),\n",
    "            \"num_cols\": self.num_cols_,\n",
    "            \"cat_cols\": self.cat_cols_,\n",
    "            \"is_supervised\": self.is_supervised_,\n",
    "            \"task\": self.task_,\n",
    "            \"n_samples\": self.n_samples_,\n",
    "            \"n_features\": self.n_features_,\n",
    "            \"Z_scaler\": self.Z_scaler_,\n",
    "        }\n",
    "        with open(path.with_suffix(\".json\"), \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        import numpy as _np\n",
    "        _np.savez_compressed(path.with_suffix(\".npz\"),\n",
    "                             K=self.K_, evals=self.evals_, evecs=self.evecs_,\n",
    "                             Z_train=self.Z_train_, X_train_pre=self.X_train_pre_)\n",
    "        try:\n",
    "            import joblib\n",
    "            joblib.dump({\"preprocessor\": self.preprocessor_, \"forest\": self.forest_, \"estimators\": self.estimators_},\n",
    "                        path.with_suffix(\".skjoblib\"))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to save sklearn objects with joblib: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: Union[str, 'Path']) -> \"RFAE\":\n",
    "        from pathlib import Path as _Path\n",
    "        path = _Path(path)\n",
    "        with open(path.with_suffix(\".json\"), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        cfg = RFAEConfig(**meta[\"cfg\"])\n",
    "        model = RFAE(cfg)\n",
    "        model.num_cols_ = meta[\"num_cols\"]\n",
    "        model.cat_cols_ = meta[\"cat_cols\"]\n",
    "        model.is_supervised_ = meta[\"is_supervised\"]\n",
    "        model.task_ = meta[\"task\"]\n",
    "        model.n_samples_ = meta[\"n_samples\"]\n",
    "        model.n_features_ = meta[\"n_features\"]\n",
    "        model.Z_scaler_ = meta[\"Z_scaler\"]\n",
    "        blob = np.load(path.with_suffix(\".npz\"))\n",
    "        model.K_ = blob[\"K\"]; model.evals_ = blob[\"evals\"]; model.evecs_ = blob[\"evecs\"]\n",
    "        model.Z_train_ = blob[\"Z_train\"]; model.X_train_pre_ = blob[\"X_train_pre\"]\n",
    "        try:\n",
    "            import joblib\n",
    "            objs = joblib.load(path.with_suffix(\".skjoblib\"))\n",
    "            model.preprocessor_ = objs[\"preprocessor\"]\n",
    "            model.forest_ = objs[\"forest\"]\n",
    "            model.estimators_ = objs.get(\"estimators\") or getattr(model.forest_, \"estimators_\", [])\n",
    "            model._leaf_index_per_tree_.clear(); model._leaf_size_per_tree_.clear()\n",
    "            leaves_all = [est.apply(model.X_train_pre_) for est in model.estimators_]\n",
    "            for leaves in leaves_all:\n",
    "                leaf_to_idx = {int(lid): np.where(leaves == lid)[0] for lid in np.unique(leaves)}\n",
    "                model._leaf_index_per_tree_.append(leaf_to_idx)\n",
    "                model._leaf_size_per_tree_.append({lid: len(idx) for lid, idx in leaf_to_idx.items()})\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to load sklearn objects with joblib: {e}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# ---- Evaluation helper ----\n",
    "def reconstruction_distortion(\n",
    "    X_true: Union[pd.DataFrame, np.ndarray],\n",
    "    X_recon: Union[pd.DataFrame, np.ndarray],\n",
    ") -> Dict[str, Any]:\n",
    "    if isinstance(X_true, np.ndarray):\n",
    "        X_true_df = pd.DataFrame(X_true, columns=[f\"x{i}\" for i in range(X_true.shape[1])])\n",
    "    else:\n",
    "        X_true_df = X_true.copy()\n",
    "    if isinstance(X_recon, np.ndarray):\n",
    "        X_recon_df = pd.DataFrame(X_recon, columns=X_true_df.columns)\n",
    "    else:\n",
    "        X_recon_df = X_recon.copy()\n",
    "    num_cols = [c for c in X_true_df.columns if pd.api.types.is_numeric_dtype(X_true_df[c])]\n",
    "    cat_cols = [c for c in X_true_df.columns if c not in num_cols]\n",
    "\n",
    "    cont_scores = []\n",
    "    for c in num_cols:\n",
    "        x = X_true_df[c].to_numpy(dtype=float)\n",
    "        y = X_recon_df[c].to_numpy(dtype=float)\n",
    "        var = np.var(x)\n",
    "        if var < 1e-12: continue\n",
    "        mse = np.mean((x - y) ** 2)\n",
    "        cont_scores.append(mse / var)\n",
    "    cont_mean = float(np.mean(cont_scores)) if cont_scores else np.nan\n",
    "\n",
    "    cat_scores = []\n",
    "    for c in cat_cols:\n",
    "        x = X_true_df[c].astype(str).to_numpy()\n",
    "        y = X_recon_df[c].astype(str).to_numpy()\n",
    "        cat_scores.append(float(np.mean(x != y)))\n",
    "    cat_mean = float(np.mean(cat_scores)) if cat_scores else np.nan\n",
    "\n",
    "    parts = [v for v in [cont_mean, cat_mean] if not np.isnan(v)]\n",
    "    overall = float(np.mean(parts)) if parts else np.nan\n",
    "    return {\"continuous\": cont_mean, \"categorical\": cat_mean, \"overall\": overall}\n",
    "\n",
    "\n",
    "# ---- Optional: neural encoder (Torch) ----\n",
    "if TORCH_AVAILABLE:\n",
    "    class _MLP(torch.nn.Module):\n",
    "        def __init__(self, in_dim: int, out_dim: int, hidden: List[int]):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            last = in_dim\n",
    "            for h in hidden:\n",
    "                layers += [torch.nn.Linear(last, h), torch.nn.ReLU()]\n",
    "                last = h\n",
    "            layers += [torch.nn.Linear(last, out_dim)]\n",
    "            self.net = torch.nn.Sequential(*layers)\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    def _to_tensor(x: np.ndarray, device: str, dtype):\n",
    "        return torch.tensor(x, device=device, dtype=dtype)\n",
    "\n",
    "    def _from_tensor(x: \"torch.Tensor\") -> np.ndarray:\n",
    "        return x.detach().cpu().numpy()\n",
    "\n",
    "    def fit_neural_encoder(self: \"RFAE\",\n",
    "                           hidden: List[int] = [256, 128],\n",
    "                           epochs: int = 50,\n",
    "                           batch_size: int = 256,\n",
    "                           lr: float = 1e-3,\n",
    "                           weight_decay: float = 1e-6,\n",
    "                           patience: int = 10,\n",
    "                           verbose: bool = True) -> \"RFAE\":\n",
    "        check_is_fitted(self, [\"Z_train_\", \"X_train_pre_\"])\n",
    "        device = self.cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        dtype = torch.float32 if self.cfg.float_dtype == \"float32\" else torch.float64\n",
    "        model = _MLP(self.n_features_, self.cfg.latent_dim, hidden).to(device=device, dtype=dtype)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        X = _to_tensor(self.X_train_pre_, device, dtype)\n",
    "        Z = _to_tensor(self.Z_train_, device, dtype)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        idx = np.arange(n)\n",
    "        loss_hist: List[float] = []\n",
    "\n",
    "        def early_stop(hist, p):\n",
    "            if len(hist) < p + 1: return False\n",
    "            best = min(hist[:-p])\n",
    "            return all(l >= best for l in hist[-p:])\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            self._rng.shuffle(idx)\n",
    "            model.train(); ep_loss = 0.0\n",
    "            for s in range(0, n, batch_size):\n",
    "                b = idx[s:s+batch_size]\n",
    "                pred = model(X[b]); loss = loss_fn(pred, Z[b])\n",
    "                opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "                ep_loss += float(loss.detach().cpu().item()) * len(b)\n",
    "            ep_loss /= float(n)\n",
    "            loss_hist.append(ep_loss)\n",
    "            if verbose: print(f\"[neural-encoder] epoch {ep:03d}/{epochs}  loss={ep_loss:.6f}\")\n",
    "            if early_stop(loss_hist, patience):\n",
    "                if verbose: print(\"[neural-encoder] early stopping\")\n",
    "                break\n",
    "\n",
    "        self._nn_encoder = model\n",
    "        self._nn_device = device\n",
    "        self._nn_dtype = dtype\n",
    "        return self\n",
    "\n",
    "    def encode_nn(self: \"RFAE\", X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        if not hasattr(self, \"_nn_encoder\"):\n",
    "            raise RuntimeError(\"Neural encoder not trained. Call fit_neural_encoder first.\")\n",
    "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.X_train_df_.columns)\n",
    "        Xp = np.asarray(self.preprocessor_.transform(X_df), dtype=self.cfg.float_dtype)\n",
    "        self._nn_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            z = self._nn_encoder(_to_tensor(Xp, self._nn_device, self._nn_dtype))\n",
    "        return _from_tensor(z)\n",
    "\n",
    "    setattr(RFAE, \"fit_neural_encoder\", fit_neural_encoder)\n",
    "    setattr(RFAE, \"encode_nn\", encode_nn)\n",
    "\n",
    "\n",
    "def _demo_iris():\n",
    "    from sklearn.datasets import load_iris\n",
    "    data = load_iris(as_frame=True)\n",
    "    X = data.frame.drop(columns=[\"target\"]); y = data.frame[\"target\"]\n",
    "    cfg = RFAEConfig(mode=\"supervised-classification\", n_estimators=300, latent_dim=4, use_torch=False, verbose=True)\n",
    "    rfae = RFAE(cfg).fit(X, y)\n",
    "    Z = rfae.encode(X.iloc[:5])\n",
    "    Xh = rfae.decode(Z)\n",
    "    print(\"Z shape:\", Z.shape); print(Xh.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo_iris()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dbc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ðŸ“š Import the module\n",
    "from rfae import RFAE, RFAEConfig, reconstruction_distortion\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "print(\"âœ… rfae.py imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de31870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ðŸ“ˆ Helper: scatter plot for embeddings\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "def plot_embedding(Z, labels=None, title=\"Diffusion Embedding (RFAE)\"):\n",
    "    Z = np.asarray(Z)\n",
    "    if Z.shape[1] < 2: raise ValueError(\"Need at least 2D latent space to plot.\")\n",
    "    plt.figure(figsize=(6,5))\n",
    "    if labels is None:\n",
    "        plt.scatter(Z[:,0], Z[:,1], s=16, alpha=0.8)\n",
    "    else:\n",
    "        labels = np.asarray(labels)\n",
    "        for u in np.unique(labels):\n",
    "            m = (labels == u)\n",
    "            plt.scatter(Z[m,0], Z[m,1], s=16, alpha=0.8, label=str(u))\n",
    "        plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.title(title); plt.xlabel(\"Z1\"); plt.ylabel(\"Z2\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e1de4",
   "metadata": {},
   "source": [
    "## ðŸŒ¸ Supervised demo (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.frame.drop(columns=[\"target\"]); y = data.frame[\"target\"]\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "cfg = RFAEConfig(mode=\"supervised-classification\", n_estimators=500, latent_dim=8, diffusion_time=1.0, k_neighbors=40, use_torch=True, verbose=True)\n",
    "\n",
    "t0 = time(); model = RFAE(cfg).fit(Xtr, ytr); fit_time = time()-t0\n",
    "Ztr = model.encode(Xtr); Zte = model.encode(Xte)\n",
    "Xhat = model.decode(Zte); dist = reconstruction_distortion(Xte, Xhat)\n",
    "\n",
    "print(f\"âœ… Fit time: {fit_time:.2f}s\")\n",
    "print(\"ðŸ“Š Distortion (lower is better):\", dist)\n",
    "plot_embedding(np.vstack([Ztr[:,:2], Zte[:,:2]]), labels=np.hstack([ytr.values, yte.values]), title=\"Iris â€” RFAE diffusion embedding (2D)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c09f1",
   "metadata": {},
   "source": [
    "### ðŸ’¾ Save & reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"rfae_iris\")\n",
    "reloaded = RFAE.load(\"rfae_iris\")\n",
    "Zte2 = reloaded.encode(Xte)\n",
    "print(\"Î” encoding L2 between pre/post load:\", float(np.linalg.norm(Zte - Zte2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1918126",
   "metadata": {},
   "source": [
    "## ðŸŒ™ Unsupervised demo (Two Moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons(n_samples=1500, noise=0.07, random_state=42)\n",
    "import pandas as pd\n",
    "X_moons = pd.DataFrame(X_moons, columns=[\"x1\",\"x2\"])\n",
    "\n",
    "cfg_unsup = RFAEConfig(mode=\"unsupervised\", n_estimators=800, latent_dim=8, k_neighbors=50, use_torch=True, verbose=False)\n",
    "unsup = RFAE(cfg_unsup).fit(X_moons)\n",
    "Z_moons = unsup.encode(X_moons)\n",
    "plot_embedding(Z_moons[:, :2], labels=y_moons, title=\"Unsupervised RFAE on Two Moons\")\n",
    "Xhat_moons = unsup.decode(Z_moons)\n",
    "Xhat_moons.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0c103",
   "metadata": {},
   "source": [
    "## âš¡ Optional: neural encoder to approximate NystrÃ¶m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit_neural_encoder(hidden=[256,128], epochs=40, lr=1e-3, patience=6, verbose=True)\n",
    "from time import time\n",
    "t0 = time(); Z_fast = model.encode_nn(Xte); t_fast = time()-t0\n",
    "t0 = time(); Z_ref  = model.encode(Xte);  t_ref  = time()-t0\n",
    "approx_err = float(np.linalg.norm(Z_fast - Z_ref) / (np.linalg.norm(Z_ref)+1e-8))\n",
    "print(f\"â±ï¸ encode_nn: {t_fast*1000:.1f} ms vs NystrÃ¶m: {t_ref*1000:.1f} ms; relative error ~ {approx_err:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4fe4a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Notes\n",
    "- RF kernel is doubly-stochastic by averaging 1/leaf-size co-membership over trees.\n",
    "- Diffusion maps on the RF kernel give the encoder; NystrÃ¶m handles out-of-sample.\n",
    "- k-NN decoder (weighted average + categorical vote) is the fast/accurate default.\n",
    "- For large `n`, the dense kernel is O(n^2) memory; sub-sample or use approximate eigensolvers.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}